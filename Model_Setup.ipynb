{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## You are using the Python ARM Radar Toolkit (Py-ART), an open source\n",
      "## library for working with weather radar data. Py-ART is partly\n",
      "## supported by the U.S. Department of Energy as part of the Atmospheric\n",
      "## Radiation Measurement (ARM) Climate Research Facility, an Office of\n",
      "## Science user facility.\n",
      "##\n",
      "## If you use this software to prepare a publication, please cite:\n",
      "##\n",
      "##     JJ Helmus and SM Collis, JORS 2016, doi: 10.5334/jors.119\n",
      "\n",
      "Pysteps configuration file found at: /scratch/mch/fackerma/miniforge3/envs/testenv/lib/python3.12/site-packages/pysteps/pystepsrc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import pyart\n",
    "import glob\n",
    "import matplotlib.patheffects as path_effects\n",
    "from matplotlib.colors import BoundaryNorm, LinearSegmentedColormap\n",
    "from pyproj import Transformer\n",
    "import radlib\n",
    "import os\n",
    "import h5py\n",
    "import matplotlib.dates as mdates\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from cartopy.feature import NaturalEarthFeature\n",
    "import xmltodict, geopandas, geojson, xml, json #xml and json do not exist\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import geopy.distance\n",
    "import numpy.matlib as npm\n",
    "import copy\n",
    "from scipy.signal import convolve2d\n",
    "from astropy.convolution import convolve\n",
    "import scipy.ndimage as ndi\n",
    "import re\n",
    "from skimage.draw import polygon\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "from pysteps import io, motion, rcparams\n",
    "from pysteps.utils import conversion, transformation\n",
    "from pysteps.visualization import plot_precip_field, quiver\n",
    "\n",
    "import json\n",
    "\n",
    "from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "import rasterio\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "\n",
    "from geopandas import GeoDataFrame\n",
    "import datetime\n",
    "\n",
    "\n",
    "os.environ[\"library_metranet_path\"] = \"/store_new/mch/msrad/idl/lib/radlib4/\" # needed for pyradlib\n",
    "os.environ[\"METRANETLIB_PATH\"] = \"/store_new/mch/msrad/idl/lib/radlib4/\" # needed for pyart_mch\n",
    "\n",
    "\n",
    "# Calculate Swiss grid coordinates into composite raster points\n",
    "def swiss_to_grid_index(swiss_x, swiss_y, clons, clats, zh_shape):\n",
    "    # Initialize transformers\n",
    "    transformer_swiss_to_3035 = Transformer.from_crs(21781, 3035, always_xy=True)\n",
    "    \n",
    "    # Transform Swiss coordinates to EPSG:3035\n",
    "    x_3035, y_3035 = transformer_swiss_to_3035.transform(swiss_x, swiss_y)\n",
    "    \n",
    "    # Calculate distances\n",
    "    distances = np.sqrt((clons - x_3035)**2 + (clats - y_3035)**2)\n",
    "    \n",
    "    # Find the index of the minimum distance\n",
    "    y_idx, x_idx = np.unravel_index(np.argmin(distances), distances.shape)\n",
    "    \n",
    "    # Create a function to get values at specific vertical levels\n",
    "    def get_value_at_level(zh_array, level):\n",
    "        if 0 <= level < zh_shape[2]:\n",
    "            return zh_array[y_idx, x_idx, level]\n",
    "        else:\n",
    "            raise ValueError(f\"Level must be between 0 and {zh_shape[2]-1}\")\n",
    "    \n",
    "    return y_idx, x_idx, get_value_at_level\n",
    "\n",
    "# Function to retrieve cross sections for all variables of the wind composite\n",
    "def load_and_create_cross_sections(year, month, day, valid_time):\n",
    "    # Load the .npz file\n",
    "    data = np.load(f'/scratch/mch/fackerma/orders/full_composite_npz/{year}{month}{day}{valid_time}00_conv_wind_composite_data.npz')\n",
    "    \n",
    "    # Access the specific arrays\n",
    "    ZH = data['ZH_max']\n",
    "    rad_shear = data['RAD_SHEAR_LLSD_max']\n",
    "    az_shear = data['AZ_SHEAR_LLSD_abs_max']\n",
    "    RVEL = data['RVEL_DE_abs_max']\n",
    "    KDP = data['KDP_max']\n",
    "    ZDR = data['ZH_max']\n",
    "    \n",
    "    # Create cross-sections and 2D projections for each array\n",
    "    arrays = [ZH, rad_shear, az_shear, RVEL, KDP, ZDR]\n",
    "    names = ['ZH', 'rad_shear', 'az_shear', 'RVEL', 'KDP', 'ZDR']\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for arr, name in zip(arrays, names):\n",
    "        # Create cross-sections\n",
    "        results[f'{name}_x_cross'] = arr[y_idx, x_start:x_end, :]\n",
    "        results[f'{name}_y_cross'] = arr[y_start:y_end, x_idx, :]\n",
    "        \n",
    "        # Create 2D max projection\n",
    "        results[f'{name}_2d'] = np.nanmax(arr, axis=2)\n",
    "    \n",
    "    return results\n",
    "# All results are stored in a dictionary, with keys formatted as '{name}_x_cross', '{name}_y_cross', and '{name}_2d'\n",
    "\n",
    "\n",
    "# Define the Swiss grid (adjusted to match data dimensions)\n",
    "chx = np.arange(255000, 255000 + 710 * 1000, 1000)  # Easting values (710 points)\n",
    "chy = sorted(np.arange(-160000, -160000 + 640 * 1000, 1000), reverse=True)  # Northing values (640 points)\n",
    "X, Y = np.meshgrid(chx, chy)\n",
    "\n",
    "# Initialize transformer for Swiss grid to WGS84 (EPSG:21781 to EPSG:4326 PlateCarree)\n",
    "transformer = Transformer.from_crs(21781, 4326, always_xy=True)\n",
    "clons, clats = transformer.transform(X, Y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TRT_2019_05-10.pkl...\n",
      "Loading TRT_2020_05-10.pkl...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TRT_2021_05-10.pkl...\n",
      "Loading TRT_2022_05-10.pkl...\n",
      "Loading TRT_2023_05-10.pkl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mch/fackerma/miniforge3/envs/testenv/lib/python3.12/site-packages/geopandas/array.py:1638: UserWarning: CRS not set for some of the concatenation inputs. Setting output's CRS as WGS 84 (the single non-null crs provided).\n",
      "  return GeometryArray(data, crs=_get_common_crs(to_concat))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merged dataframe shape: (2240266, 91)\n"
     ]
    }
   ],
   "source": [
    "# Import the data\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the base directory and file names\n",
    "base_dir = \"/scratch/mch/fackerma/orders/TRT_processing_2/\"\n",
    "yearly_files = [\n",
    "    \"TRT_2019_05-10.pkl\",\n",
    "    \"TRT_2020_05-10.pkl\",\n",
    "    \"TRT_2021_05-10.pkl\",\n",
    "    \"TRT_2022_05-10.pkl\",\n",
    "    \"TRT_2023_05-10.pkl\",\n",
    "]\n",
    "\n",
    "# Load and merge dataframes\n",
    "dfs = []\n",
    "for file_name in yearly_files:\n",
    "    file_path = os.path.join(base_dir, file_name)\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Loading {file_name}...\")\n",
    "        df = pd.read_pickle(file_path)\n",
    "        dfs.append(df)\n",
    "    else:\n",
    "        print(f\"⚠️ File not found: {file_name}\")\n",
    "\n",
    "if not dfs:\n",
    "    print(\"No data loaded. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"\\nMerged dataframe shape: {merged_df.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered DataFrame:\n",
      "                timestamp                                           geometry  \\\n",
      "65788 2019-06-15 15:35:00  POLYGON ((6.8781 45.9107, 6.8653 45.9016, 6.86...   \n",
      "65789 2019-06-15 15:35:00  POLYGON ((8.1332 46.3148, 8.1201 46.3059, 8.11...   \n",
      "65790 2019-06-15 15:35:00  POLYGON ((6.4614 46.1507, 6.4616 46.1418, 6.44...   \n",
      "65791 2019-06-15 15:35:00  POLYGON ((8.1177 46.117, 8.1047 46.108, 8.0918...   \n",
      "65792 2019-06-15 15:35:00  POLYGON ((8.1562 46.0897, 8.1432 46.0808, 8.14...   \n",
      "65793 2019-06-15 15:35:00  POLYGON ((8.4001 45.989, 8.387 45.9801, 8.3866...   \n",
      "65794 2019-06-15 15:35:00  POLYGON ((6.9303 45.839, 6.9304 45.83, 6.9176 ...   \n",
      "65795 2019-06-15 15:35:00  POLYGON ((8.2774 46.4038, 8.2642 46.3949, 8.25...   \n",
      "65796 2019-06-15 15:35:00  POLYGON ((7.5479 45.7501, 7.5478 45.7142, 7.56...   \n",
      "65797 2019-06-15 15:35:00  POLYGON ((5.7504 45.7015, 5.7506 45.6925, 5.73...   \n",
      "65798 2019-06-15 15:35:00  POLYGON ((6.1868 45.7072, 6.1874 45.6802, 6.16...   \n",
      "65799 2019-06-15 15:35:00  POLYGON ((7.522 45.6152, 7.5091 45.6063, 7.496...   \n",
      "65800 2019-06-15 15:35:00  POLYGON ((7.1776 45.2552, 7.1395 45.2281, 7.12...   \n",
      "65801 2019-06-15 15:35:00  POLYGON ((5.9102 44.5704, 5.8732 44.5429, 5.86...   \n",
      "65802 2019-06-15 15:35:00  POLYGON ((6.7919 44.4713, 6.767 44.4531, 6.767...   \n",
      "65803 2019-06-15 15:35:00  POLYGON ((7.9209 45.8031, 7.8951 45.7852, 7.88...   \n",
      "65804 2019-06-15 15:35:00  POLYGON ((7.068 46.4152, 7.0681 46.3883, 7.055...   \n",
      "65805 2019-06-15 15:35:00  POLYGON ((5.197 44.4683, 5.1977 44.4503, 5.185...   \n",
      "65806 2019-06-15 15:35:00  POLYGON ((7.3799 46.5688, 7.38 46.5598, 7.3669...   \n",
      "65807 2019-06-15 15:35:00  POLYGON ((7.5755 46.5417, 7.5755 46.5237, 7.56...   \n",
      "65808 2019-06-15 15:35:00  POLYGON ((6.491 46.6818, 6.4781 46.6727, 6.465...   \n",
      "65809 2019-06-15 15:35:00  POLYGON ((6.0879 46.5967, 6.0883 46.5788, 6.07...   \n",
      "65810 2019-06-15 15:35:00  POLYGON ((6.926 46.2977, 6.9131 46.2887, 6.913...   \n",
      "65811 2019-06-15 15:35:00  POLYGON ((6.1939 46.526, 6.1555 46.4986, 6.156...   \n",
      "65812 2019-06-15 15:35:00  POLYGON ((5.4447 47.757, 5.445 47.748, 5.432 4...   \n",
      "65813 2019-06-15 15:35:00  POLYGON ((5.2466 47.7084, 5.2469 47.6994, 5.23...   \n",
      "65814 2019-06-15 15:35:00  POLYGON ((5.4252 47.5677, 5.4122 47.5585, 5.41...   \n",
      "65815 2019-06-15 15:35:00  POLYGON ((6.9659 46.19, 6.953 46.1809, 6.9141 ...   \n",
      "65816 2019-06-15 15:35:00  POLYGON ((11 47.4494, 10.997 47.4045, 11.0096 ...   \n",
      "65817 2019-06-15 15:35:00  POLYGON ((10.3756 47.197, 10.3619 47.1883, 10....   \n",
      "65818 2019-06-15 15:35:00  POLYGON ((8.0256 47.17, 7.9857 47.1432, 7.9726...   \n",
      "65819 2019-06-15 15:35:00  POLYGON ((7.7865 46.9101, 7.7864 46.8921, 7.77...   \n",
      "65820 2019-06-15 15:35:00  POLYGON ((11.7819 46.783, 11.7783 46.7381, 11....   \n",
      "65821 2019-06-15 15:35:00  POLYGON ((11.0618 46.7992, 11.0612 46.7902, 11...   \n",
      "65822 2019-06-15 15:35:00  POLYGON ((10.428 46.7185, 10.427 46.7005, 10.4...   \n",
      "65823 2019-06-15 15:35:00  POLYGON ((7.5516 47.5761, 7.5383 47.5672, 7.53...   \n",
      "\n",
      "      CS Marker STA Marker ESWD Marker Gust_Flag             traj_ID  \\\n",
      "65788         0          0           0         -  2019061515350115.0   \n",
      "65789         0          0           0         -  2019061515200086.0   \n",
      "65790         0          0           0         -  2019061515150101.0   \n",
      "65791         0          0           0         -  2019061515250100.0   \n",
      "65792         0          0           0         -  2019061515200090.0   \n",
      "65793         0          0           0         -  2019061515350113.0   \n",
      "65794         0          0           0         -  2019061515350117.0   \n",
      "65795         0          0           0         -  2019061515350107.0   \n",
      "65796         0          0           0         -  2019061515350120.0   \n",
      "65797         0          0           0         -  2019061515250111.0   \n",
      "65798         0          0           0         -  2019061515250109.0   \n",
      "65799         0          0           0         -  2019061514500089.0   \n",
      "65800         0          0           0         -  2019061515200111.0   \n",
      "65801         0          0           0         -  2019061515250121.0   \n",
      "65802         0          0           0         -  2019061515300126.0   \n",
      "65803         0          0           0         -  2019061515300111.0   \n",
      "65804         0          0           0         -  2019061515350106.0   \n",
      "65805         0          0           0         -  2019061515000118.0   \n",
      "65806         0          0           0         -  2019061514550083.0   \n",
      "65807         0          0           0         -  2019061515300099.0   \n",
      "65808         0          4           0       Yes  2019061509450011.0   \n",
      "65809         0          0           0         -  2019061515250030.0   \n",
      "65810         0          0           0         -  2019061515350046.0   \n",
      "65811         0          0           0         -  2019061515300036.0   \n",
      "65812         0          0           0         -  2019061515350087.0   \n",
      "65813         0          0           0         -  2019061515050078.0   \n",
      "65814         0          0           0         -  2019061515200068.0   \n",
      "65815         0          0           0         -  2019061510350021.0   \n",
      "65816         0          0           0         -  2019061515300087.0   \n",
      "65817         0          0           0         -  2019061515350094.0   \n",
      "65818         0          0           0         -  2019061512350048.0   \n",
      "65819         0          0           0         -  2019061513550078.0   \n",
      "65820         0          0           0         -  2019061515250090.0   \n",
      "65821         0          0           0         -  2019061515350100.0   \n",
      "65822         0          0           0         -  2019061515250092.0   \n",
      "65823         0          0           0         -  2019061514350064.0   \n",
      "\n",
      "               time      lon      lat  ... nrPOHthr010 nrPOHthr020  \\\n",
      "65788  2.019062e+11   6.9142  45.8717  ...         NaN         NaN   \n",
      "65789  2.019062e+11   8.1716  46.2555  ...         NaN         NaN   \n",
      "65790  2.019062e+11   6.4247  45.9524  ...         NaN         NaN   \n",
      "65791  2.019062e+11   8.0943  46.0658  ...         NaN         NaN   \n",
      "65792  2.019062e+11   8.1605  46.0422  ...         NaN         NaN   \n",
      "65793  2.019062e+11   8.4498  45.9284  ...         NaN         NaN   \n",
      "65794  2.019062e+11   7.0031  45.7754  ...         NaN         NaN   \n",
      "65795  2.019062e+11    8.301  46.3541  ...         NaN         NaN   \n",
      "65796  2.019062e+11   7.5925  45.7134  ...         NaN         NaN   \n",
      "65797  2.019062e+11   5.7945  45.6306  ...         NaN         NaN   \n",
      "65798  2.019062e+11    6.222  45.6636  ...         NaN         NaN   \n",
      "65799  2.019062e+11   7.4745  45.4964  ...         NaN         NaN   \n",
      "65800  2.019062e+11    7.148  45.1726  ...         NaN         NaN   \n",
      "65801  2.019062e+11   5.9475  44.5398  ...         NaN         NaN   \n",
      "65802  2.019062e+11   6.8067  44.4223  ...         NaN         NaN   \n",
      "65803  2.019062e+11   7.9191   45.775  ...         NaN         NaN   \n",
      "65804  2.019062e+11    7.079    46.36  ...         NaN         NaN   \n",
      "65805  2.019062e+11   5.2437  44.4191  ...         NaN         NaN   \n",
      "65806  2.019062e+11   7.3171  46.4377  ...         NaN         NaN   \n",
      "65807  2.019062e+11   7.6225  46.4698  ...         NaN         NaN   \n",
      "65808  2.019062e+11   6.6687  46.5148  ...         NaN         NaN   \n",
      "65809  2.019062e+11   6.1075  46.5224  ...         NaN         NaN   \n",
      "65810  2.019062e+11   6.9437  46.2498  ...         NaN         NaN   \n",
      "65811  2.019062e+11   6.2262  46.4599  ...         NaN         NaN   \n",
      "65812  2.019062e+11   5.4567  47.6822  ...         NaN         NaN   \n",
      "65813  2.019062e+11   5.2194  47.6416  ...         NaN         NaN   \n",
      "65814  2.019062e+11   5.4312  47.4726  ...         NaN         NaN   \n",
      "65815  2.019062e+11   7.0035  46.0904  ...         NaN         NaN   \n",
      "65816  2.019062e+11  11.0355   47.414  ...         NaN         NaN   \n",
      "65817  2.019062e+11  10.3966  47.1673  ...         NaN         NaN   \n",
      "65818  2.019062e+11   7.9877  47.0477  ...         NaN         NaN   \n",
      "65819  2.019062e+11   7.7982  46.8351  ...         NaN         NaN   \n",
      "65820  2.019062e+11  11.7896  46.7047  ...         NaN         NaN   \n",
      "65821  2.019062e+11  11.0695  46.7597  ...         NaN         NaN   \n",
      "65822  2.019062e+11  10.4592   46.687  ...         NaN         NaN   \n",
      "65823  2.019062e+11   7.5133  47.4941  ...         NaN         NaN   \n",
      "\n",
      "      nrPOHthr030 nrPOHthr040 nrPOHthr050 nrPOHthr060 nrPOHthr070 nrPOHthr080  \\\n",
      "65788         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "65789         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "65790         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "65791         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "65792         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "65793         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "65794         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "65795         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "65796         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "65797         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "65798         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "65799         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "65800         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "65801         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "65802         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "65803         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "65804         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "65805         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "65806         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "65807         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "65808         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "65809         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "65810         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "65811         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "65812         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "65813         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "65814         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "65815         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "65816         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "65817         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "65818         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "65819         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "65820         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "65821         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "65822         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "65823         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "\n",
      "      nrPOHthr090 nrPOHthr100  \n",
      "65788         NaN         NaN  \n",
      "65789         NaN         NaN  \n",
      "65790         NaN         NaN  \n",
      "65791         NaN         NaN  \n",
      "65792         NaN         NaN  \n",
      "65793         NaN         NaN  \n",
      "65794         NaN         NaN  \n",
      "65795         NaN         NaN  \n",
      "65796         NaN         NaN  \n",
      "65797         NaN         NaN  \n",
      "65798         NaN         NaN  \n",
      "65799         NaN         NaN  \n",
      "65800         NaN         NaN  \n",
      "65801         NaN         NaN  \n",
      "65802         NaN         NaN  \n",
      "65803         NaN         NaN  \n",
      "65804         NaN         NaN  \n",
      "65805         NaN         NaN  \n",
      "65806         NaN         NaN  \n",
      "65807         NaN         NaN  \n",
      "65808         NaN         NaN  \n",
      "65809         NaN         NaN  \n",
      "65810         NaN         NaN  \n",
      "65811         NaN         NaN  \n",
      "65812         NaN         NaN  \n",
      "65813         NaN         NaN  \n",
      "65814         NaN         NaN  \n",
      "65815         NaN         NaN  \n",
      "65816         NaN         NaN  \n",
      "65817         NaN         NaN  \n",
      "65818         NaN         NaN  \n",
      "65819         NaN         NaN  \n",
      "65820         NaN         NaN  \n",
      "65821         NaN         NaN  \n",
      "65822         NaN         NaN  \n",
      "65823         NaN         NaN  \n",
      "\n",
      "[36 rows x 91 columns]\n",
      "\n",
      "Loaded Arrays:\n",
      "ZH: (640, 710, 93)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyproj import Transformer\n",
    "\n",
    "from pyproj import Transformer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define the Swiss grid (adjusted to match data dimensions)\n",
    "chx = np.arange(255000, 255000 + 710 * 1000, 1000)  # Easting values (710 points)\n",
    "chy = sorted(np.arange(-160000, -160000 + 640 * 1000, 1000), reverse=True)  # Northing values (640 points)\n",
    "X, Y = np.meshgrid(chx, chy)\n",
    "\n",
    "# Initialize transformer for Swiss grid to WGS84 (EPSG:21781 to EPSG:4326 PlateCarree)\n",
    "transformer = Transformer.from_crs(21781, 4326, always_xy=True)\n",
    "clons, clats = transformer.transform(X, Y)\n",
    "\n",
    "\n",
    "\n",
    "def filter_rows_by_datetime(merged_df, datetime_str):\n",
    "    # Ensure the 'timestamp' column is in datetime format\n",
    "    merged_df['timestamp'] = pd.to_datetime(merged_df['timestamp'])\n",
    "    \n",
    "    # Convert input datetime string to a datetime object\n",
    "    target_datetime = pd.to_datetime(datetime_str)\n",
    "    \n",
    "    # Filter rows for the exact datetime\n",
    "    filtered_df = merged_df[merged_df['timestamp'] == target_datetime]\n",
    "    return filtered_df\n",
    "\n",
    "def load_npz_file(datetime_obj):\n",
    "    # Extract components from datetime\n",
    "    year = datetime_obj.strftime('%Y')\n",
    "    month = datetime_obj.strftime('%m')\n",
    "    day = datetime_obj.strftime('%d')\n",
    "    valid_time = datetime_obj.strftime('%H%M')  # Format: '1535' for 15:35\n",
    "    \n",
    "    # Load .npz file\n",
    "    file_path = f'/scratch/mch/maregger/hailclass/convective_wind/full_composite_npz/{year}{month}{day}{valid_time}00_conv_wind_composite_data.npz'\n",
    "    return np.load(file_path)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define your datetime (including time)\n",
    "    datetime_str = '2019-06-15 15:35:00'\n",
    "    converted = datetime.strptime(datetime_str, '%Y-%m-%d %H:%M:%S').strftime('%Y%m%d%H%M')\n",
    "    \n",
    "    # Filter rows by exact datetime\n",
    "    filtered_df = filter_rows_by_datetime(merged_df, datetime_str)\n",
    "    print(\"Filtered DataFrame:\")\n",
    "    print(filtered_df)\n",
    "    \n",
    "    # Load corresponding .npz file\n",
    "    if not filtered_df.empty:\n",
    "        data = load_npz_file(pd.to_datetime(datetime_str))\n",
    "        ZH, rad_shear, KDP = data['ZH_max'], data['RAD_SHEAR_LLSD_max'], data['KDP_max']\n",
    "        print(\"\\nLoaded Arrays:\")\n",
    "        print(\"ZH:\", ZH.shape)  # Check array dimensions instead of printing NaNs\n",
    "\n",
    "    # Anchor the fields in the Swiss grid\n",
    "    anchored_data = {\n",
    "        'ZH': ZH,\n",
    "        'rad_shear': rad_shear,\n",
    "        'KDP': KDP,\n",
    "        'easting': X,\n",
    "        'northing': Y,\n",
    "        'longitude': clons,\n",
    "        'latitude': clats\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from shapely.prepared import prep\n",
    "from shapely.geometry import Point\n",
    "from scipy.ndimage import center_of_mass\n",
    "\n",
    "def calculate_metrics(filtered_df, clons, clats, ZH, rad_shear, KDP):\n",
    "    # Precompute grid properties\n",
    "    ny, nx = clons.shape\n",
    "    nz = ZH.shape[2]\n",
    "    clons_flat = clons.ravel()\n",
    "    clats_flat = clats.ravel()\n",
    "    \n",
    "    # Initialize results storage\n",
    "    results = {\n",
    "        'ZH_com_height': [], 'ZH_percent_above_45': [],\n",
    "        'KDP_com_height': [], 'KDP_percent_above_2': [],\n",
    "        'rad_shear_max': [], 'rad_shear_percent_above_2': []\n",
    "    }\n",
    "\n",
    "    for _, row in filtered_df.iterrows():\n",
    "        poly = row['geometry']\n",
    "        prep_poly = prep(poly)\n",
    "        minx, miny, maxx, maxy = poly.bounds\n",
    "        \n",
    "        # Bounding box optimization\n",
    "        bbox_mask = (clons_flat >= minx) & (clons_flat <= maxx) & \\\n",
    "                    (clats_flat >= miny) & (clats_flat <= maxy)\n",
    "        if not bbox_mask.any():\n",
    "            results = _append_defaults(results)\n",
    "            continue\n",
    "            \n",
    "        # Vectorized containment check\n",
    "        contained = np.array([prep_poly.contains(Point(p)) \n",
    "                               for p in zip(clons_flat[bbox_mask], clats_flat[bbox_mask])])\n",
    "        if not contained.any():\n",
    "            results = _append_defaults(results)\n",
    "            continue\n",
    "            \n",
    "        # Create 2D mask\n",
    "        mask_2d = np.zeros((ny, nx), bool)\n",
    "        yx_indices = np.unravel_index(np.where(bbox_mask)[0][contained], (ny, nx))\n",
    "        mask_2d[yx_indices] = True\n",
    "        \n",
    "        # Extend the mask to 3D by repeating along the vertical dimension\n",
    "        mask_3d = np.broadcast_to(mask_2d[..., None], (ny, nx, nz))\n",
    "        \n",
    "        # Extract values within the polygon for ZH, KDP, rad_shear\n",
    "        ZH_masked = np.where(mask_3d, ZH, np.nan)\n",
    "        KDP_masked = np.where(mask_3d, KDP, np.nan)\n",
    "        rad_shear_masked = np.where(mask_3d, rad_shear, np.nan)\n",
    "\n",
    "        # Replace NaN values with 0 for calculations\n",
    "        ZH_masked[np.isnan(ZH_masked)] = 0\n",
    "        KDP_masked[np.isnan(KDP_masked)] = 0\n",
    "        rad_shear_masked[np.isnan(rad_shear_masked)] = 0\n",
    "        \n",
    "        # Calculate metrics for ZH\n",
    "        if np.any(ZH_masked > 0):  # Check if there are valid values\n",
    "            com_ZH = center_of_mass(ZH_masked)  # Center of mass height\n",
    "            results['ZH_com_height'].append(com_ZH[2])  # Use the vertical dimension (z-axis)\n",
    "            results['ZH_percent_above_45'].append(np.sum(ZH_masked > 45) / np.size(ZH_masked) * 100)\n",
    "        else:\n",
    "            results['ZH_com_height'].append(np.nan)\n",
    "            results['ZH_percent_above_45'].append(0)\n",
    "        \n",
    "        # Calculate metrics for KDP\n",
    "        if np.any(KDP_masked > 0):  # Check if there are valid values\n",
    "            com_KDP = center_of_mass(KDP_masked)  # Center of mass height\n",
    "            results['KDP_com_height'].append(com_KDP[2])  # Use the vertical dimension (z-axis)\n",
    "            results['KDP_percent_above_2'].append(np.sum(KDP_masked > 2) / np.size(KDP_masked) * 100)\n",
    "        else:\n",
    "            results['KDP_com_height'].append(np.nan)\n",
    "            results['KDP_percent_above_2'].append(0)\n",
    "        \n",
    "        # Calculate metrics for rad_shear\n",
    "        if np.any(rad_shear_masked > 0):  # Check if there are valid values\n",
    "            results['rad_shear_max'].append(np.nanmax(rad_shear_masked))\n",
    "            results['rad_shear_percent_above_2'].append(np.sum(rad_shear_masked > 2) / np.size(rad_shear_masked) * 100)\n",
    "        else:\n",
    "            results['rad_shear_max'].append(np.nan)\n",
    "            results['rad_shear_percent_above_2'].append(0)\n",
    "\n",
    "    # Add results to DataFrame\n",
    "    for col in results:\n",
    "        filtered_df[col] = results[col]\n",
    "        \n",
    "    return filtered_df\n",
    "\n",
    "def _append_defaults(results):\n",
    "    \"\"\"Append default values to result lists when no valid data is found.\"\"\"\n",
    "    for k in results:\n",
    "        default = 0 if 'percent' in k or 'max' in k else np.nan\n",
    "        results[k].append(default)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from shapely.prepared import prep\n",
    "from shapely.geometry import Point\n",
    "from scipy.ndimage import center_of_mass\n",
    "\n",
    "def calculate_metrics(filtered_df, clons, clats, ZH, rad_shear, KDP):\n",
    "    # Precompute grid properties\n",
    "    ny, nx = clons.shape\n",
    "    nz = ZH.shape[2]\n",
    "    clons_flat = clons.ravel()\n",
    "    clats_flat = clats.ravel()\n",
    "    \n",
    "    # Initialize results storage\n",
    "    results = {\n",
    "        'ZH_com_height': [], 'ZH_percent_above_45': [],\n",
    "        'KDP_com_height': [], 'KDP_percent_above_2': [],\n",
    "        'rad_shear_max': [], 'rad_shear_percent_above_2': [],\n",
    "        'area_p': []  # New column for polygon area\n",
    "    }\n",
    "\n",
    "    for _, row in filtered_df.iterrows():\n",
    "        poly = row['geometry']\n",
    "        \n",
    "        # Calculate polygon area in square meters\n",
    "        poly_area = poly.area  # Shapely's area property calculates area in native CRS units (meters for Swiss grid)\n",
    "        results['area_p'].append(poly_area)\n",
    "        \n",
    "        prep_poly = prep(poly)\n",
    "        minx, miny, maxx, maxy = poly.bounds\n",
    "        \n",
    "        # Bounding box optimization\n",
    "        bbox_mask = (clons_flat >= minx) & (clons_flat <= maxx) & \\\n",
    "                    (clats_flat >= miny) & (clats_flat <= maxy)\n",
    "        if not bbox_mask.any():\n",
    "            results = _append_defaults(results)\n",
    "            continue\n",
    "            \n",
    "        # Vectorized containment check\n",
    "        contained = np.array([prep_poly.contains(Point(p)) \n",
    "                               for p in zip(clons_flat[bbox_mask], clats_flat[bbox_mask])])\n",
    "        if not contained.any():\n",
    "            results = _append_defaults(results)\n",
    "            continue\n",
    "            \n",
    "        # Create 2D mask\n",
    "        mask_2d = np.zeros((ny, nx), bool)\n",
    "        yx_indices = np.unravel_index(np.where(bbox_mask)[0][contained], (ny, nx))\n",
    "        mask_2d[yx_indices] = True\n",
    "        \n",
    "        # Extend the mask to 3D by repeating along the vertical dimension\n",
    "        mask_3d = np.broadcast_to(mask_2d[..., None], (ny, nx, nz))\n",
    "        \n",
    "        # Extract values within the polygon for ZH, KDP, rad_shear\n",
    "        ZH_masked = np.where(mask_3d, ZH, np.nan)\n",
    "        KDP_masked = np.where(mask_3d, KDP, np.nan)\n",
    "        rad_shear_masked = np.where(mask_3d, rad_shear, np.nan)\n",
    "\n",
    "        # Replace NaN values with 0 for calculations\n",
    "        ZH_masked[np.isnan(ZH_masked)] = 0\n",
    "        KDP_masked[np.isnan(KDP_masked)] = 0\n",
    "        rad_shear_masked[np.isnan(rad_shear_masked)] = 0\n",
    "        \n",
    "        # Calculate metrics for ZH\n",
    "        if np.any(ZH_masked > 0):  # Check if there are valid values\n",
    "            com_ZH = center_of_mass(ZH_masked)  # Center of mass height\n",
    "            results['ZH_com_height'].append(com_ZH[2])  # Use the vertical dimension (z-axis)\n",
    "            results['ZH_percent_above_45'].append(np.sum(ZH_masked > 45) / np.size(ZH_masked) * 100)\n",
    "        else:\n",
    "            results['ZH_com_height'].append(np.nan)\n",
    "            results['ZH_percent_above_45'].append(0)\n",
    "        \n",
    "        # Calculate metrics for KDP\n",
    "        if np.any(KDP_masked > 0):  # Check if there are valid values\n",
    "            com_KDP = center_of_mass(KDP_masked)  # Center of mass height\n",
    "            results['KDP_com_height'].append(com_KDP[2])  # Use the vertical dimension (z-axis)\n",
    "            results['KDP_percent_above_2'].append(np.sum(KDP_masked > 2) / np.size(KDP_masked) * 100)\n",
    "        else:\n",
    "            results['KDP_com_height'].append(np.nan)\n",
    "            results['KDP_percent_above_2'].append(0)\n",
    "        \n",
    "        # Calculate metrics for rad_shear\n",
    "        if np.any(rad_shear_masked > 0):  # Check if there are valid values\n",
    "            results['rad_shear_max'].append(np.nanmax(rad_shear_masked))\n",
    "            results['rad_shear_percent_above_2'].append(np.sum(rad_shear_masked > 2) / np.size(rad_shear_masked) * 100)\n",
    "        else:\n",
    "            results['rad_shear_max'].append(np.nan)\n",
    "            results['rad_shear_percent_above_2'].append(0)\n",
    "\n",
    "    # Add results to DataFrame\n",
    "    for col in results:\n",
    "        filtered_df[col] = results[col]\n",
    "        \n",
    "    return filtered_df\n",
    "\n",
    "def _append_defaults(results):\n",
    "    \"\"\"Append default values to result lists when no valid data is found.\"\"\"\n",
    "    for k in results:\n",
    "        default = 0 if 'percent' in k or 'max' in k else np.nan\n",
    "        results[k].append(default)\n",
    "    return results \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                timestamp                                           geometry  \\\n",
      "65788 2019-06-15 15:35:00  POLYGON ((6.8781 45.9107, 6.8653 45.9016, 6.86...   \n",
      "65789 2019-06-15 15:35:00  POLYGON ((8.1332 46.3148, 8.1201 46.3059, 8.11...   \n",
      "65790 2019-06-15 15:35:00  POLYGON ((6.4614 46.1507, 6.4616 46.1418, 6.44...   \n",
      "65791 2019-06-15 15:35:00  POLYGON ((8.1177 46.117, 8.1047 46.108, 8.0918...   \n",
      "65792 2019-06-15 15:35:00  POLYGON ((8.1562 46.0897, 8.1432 46.0808, 8.14...   \n",
      "\n",
      "      CS Marker STA Marker ESWD Marker Gust_Flag             traj_ID  \\\n",
      "65788         0          0           0         -  2019061515350115.0   \n",
      "65789         0          0           0         -  2019061515200086.0   \n",
      "65790         0          0           0         -  2019061515150101.0   \n",
      "65791         0          0           0         -  2019061515250100.0   \n",
      "65792         0          0           0         -  2019061515200090.0   \n",
      "\n",
      "               time     lon      lat  ... nrPOHthr080 nrPOHthr090 nrPOHthr100  \\\n",
      "65788  2.019062e+11  6.9142  45.8717  ...         NaN         NaN         NaN   \n",
      "65789  2.019062e+11  8.1716  46.2555  ...         NaN         NaN         NaN   \n",
      "65790  2.019062e+11  6.4247  45.9524  ...         NaN         NaN         NaN   \n",
      "65791  2.019062e+11  8.0943  46.0658  ...         NaN         NaN         NaN   \n",
      "65792  2.019062e+11  8.1605  46.0422  ...         NaN         NaN         NaN   \n",
      "\n",
      "      ZH_com_height ZH_percent_above_45 KDP_com_height KDP_percent_above_2  \\\n",
      "65788     31.935598             0.00000      31.798512                 0.0   \n",
      "65789     25.069112             0.00013      29.110096                 0.0   \n",
      "65790     24.664917             0.00004      21.751777                 0.0   \n",
      "65791     22.906389             0.00005      25.421872                 0.0   \n",
      "65792     22.619289             0.00000      23.709845                 0.0   \n",
      "\n",
      "      rad_shear_max rad_shear_percent_above_2    area_p  \n",
      "65788      4.812500                  0.000989  0.007472  \n",
      "65789      3.666667                  0.000916  0.010499  \n",
      "65790      5.213542                  0.000774  0.079174  \n",
      "65791      2.692708                  0.000263  0.005924  \n",
      "65792      2.864583                  0.000182  0.005402  \n",
      "\n",
      "[5 rows x 98 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_228818/3376967493.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df[col] = results[col]\n",
      "/tmp/ipykernel_228818/3376967493.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df[col] = results[col]\n",
      "/tmp/ipykernel_228818/3376967493.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df[col] = results[col]\n",
      "/tmp/ipykernel_228818/3376967493.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df[col] = results[col]\n",
      "/tmp/ipykernel_228818/3376967493.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df[col] = results[col]\n",
      "/tmp/ipykernel_228818/3376967493.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df[col] = results[col]\n",
      "/tmp/ipykernel_228818/3376967493.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df[col] = results[col]\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "# Assuming clons, clats, ZH, rad_shear, KDP are already defined and filtered_df contains 'geometry' column.\n",
    "new_df = calculate_metrics(filtered_df, clons, clats, ZH, rad_shear, KDP)\n",
    "\n",
    "# Display updated DataFrame with new columns\n",
    "print(new_df.head())\n",
    "\n",
    "#new_df.to_pickle(f'/scratch/mch/fackerma/orders/TRT_modelsetup_testdir/TRT_{converted}_3.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TRT_2019_05-10.pkl...\n",
      "Loading TRT_2020_05-10.pkl...\n",
      "Loading TRT_2021_05-10.pkl...\n",
      "Loading TRT_2022_05-10.pkl...\n",
      "Loading TRT_2023_05-10.pkl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mch/fackerma/miniforge3/envs/testenv/lib/python3.12/site-packages/geopandas/array.py:1638: UserWarning: CRS not set for some of the concatenation inputs. Setting output's CRS as WGS 84 (the single non-null crs provided).\n",
      "  return GeometryArray(data, crs=_get_common_crs(to_concat))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after time filtering: 416914\n",
      "Number of rows after traj_ID filtering: 33066\n",
      "Error processing 2019-05-11 12:25:00+00:00: Length of values (0) does not match length of index (1)\n",
      "Error processing 2019-05-11 12:30:00+00:00: Length of values (0) does not match length of index (2)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 211\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39mload(npz_path) \u001b[38;5;28;01mas\u001b[39;00m data:\n\u001b[0;32m--> 211\u001b[0m     ZH \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mZH_max\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    212\u001b[0m     rad_shear \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRAD_SHEAR_LLSD_max\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    213\u001b[0m     KDP \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKDP_max\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/scratch/mch/fackerma/miniforge3/envs/testenv/lib/python3.12/site-packages/numpy/lib/_npyio_impl.py:258\u001b[0m, in \u001b[0;36mNpzFile.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mMAGIC_PREFIX:\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28mbytes\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip\u001b[38;5;241m.\u001b[39mopen(key)\n\u001b[0;32m--> 258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip\u001b[38;5;241m.\u001b[39mread(key)\n",
      "File \u001b[0;32m/scratch/mch/fackerma/miniforge3/envs/testenv/lib/python3.12/site-packages/numpy/lib/format.py:858\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    856\u001b[0m             read_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(max_read_count, count \u001b[38;5;241m-\u001b[39m i)\n\u001b[1;32m    857\u001b[0m             read_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(read_count \u001b[38;5;241m*\u001b[39m dtype\u001b[38;5;241m.\u001b[39mitemsize)\n\u001b[0;32m--> 858\u001b[0m             data \u001b[38;5;241m=\u001b[39m \u001b[43m_read_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marray data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    859\u001b[0m             array[i:i\u001b[38;5;241m+\u001b[39mread_count] \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mfrombuffer(data, dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m    860\u001b[0m                                                      count\u001b[38;5;241m=\u001b[39mread_count)\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fortran_order:\n",
      "File \u001b[0;32m/scratch/mch/fackerma/miniforge3/envs/testenv/lib/python3.12/site-packages/numpy/lib/format.py:993\u001b[0m, in \u001b[0;36m_read_bytes\u001b[0;34m(fp, size, error_template)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;66;03m# io files (default in python3) return None or raise on\u001b[39;00m\n\u001b[1;32m    990\u001b[0m     \u001b[38;5;66;03m# would-block, python2 file will truncate, probably nothing can be\u001b[39;00m\n\u001b[1;32m    991\u001b[0m     \u001b[38;5;66;03m# done about that.  note that regular files can't be non-blocking\u001b[39;00m\n\u001b[1;32m    992\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 993\u001b[0m         r \u001b[38;5;241m=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    994\u001b[0m         data \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m r\n\u001b[1;32m    995\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(r) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m==\u001b[39m size:\n",
      "File \u001b[0;32m/scratch/mch/fackerma/miniforge3/envs/testenv/lib/python3.12/zipfile/__init__.py:989\u001b[0m, in \u001b[0;36mZipExtFile.read\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m n \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof:\n\u001b[0;32m--> 989\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    990\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(data):\n\u001b[1;32m    991\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_readbuffer \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m/scratch/mch/fackerma/miniforge3/envs/testenv/lib/python3.12/zipfile/__init__.py:1079\u001b[0m, in \u001b[0;36mZipExtFile._read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1077\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_left \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1078\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1079\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_crc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/scratch/mch/fackerma/miniforge3/envs/testenv/lib/python3.12/zipfile/__init__.py:1004\u001b[0m, in \u001b[0;36mZipExtFile._update_crc\u001b[0;34m(self, newdata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expected_crc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1002\u001b[0m     \u001b[38;5;66;03m# No need to compute the CRC if we don't have a reference value\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m-> 1004\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_running_crc \u001b[38;5;241m=\u001b[39m \u001b[43mcrc32\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnewdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_running_crc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;66;03m# Check the CRC if we're at the end of the file\u001b[39;00m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_running_crc \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expected_crc:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "from scipy.ndimage import center_of_mass\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyproj import Transformer\n",
    "from shapely.prepared import prep\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the Swiss grid (adjusted to match data dimensions)\n",
    "chx = np.arange(255000, 255000 + 710 * 1000, 1000)  # Easting values (710 points)\n",
    "chy = sorted(np.arange(-160000, -160000 + 640 * 1000, 1000), reverse=True)  # Northing values (640 points)\n",
    "X, Y = np.meshgrid(chx, chy)\n",
    "\n",
    "# Initialize transformer for Swiss grid to WGS84 (EPSG:21781 to EPSG:4326 PlateCarree)\n",
    "transformer = Transformer.from_crs(21781, 4326, always_xy=True)\n",
    "clons, clats = transformer.transform(X, Y)\n",
    "\n",
    "\n",
    "\n",
    "def calculate_metrics(filtered_df, clons, clats, ZH, rad_shear, KDP):\n",
    "    # Precompute grid properties\n",
    "    ny, nx = clons.shape\n",
    "    nz = ZH.shape[2]\n",
    "    clons_flat = clons.ravel()\n",
    "    clats_flat = clats.ravel()\n",
    "    \n",
    "    # Initialize results storage\n",
    "    results = {\n",
    "        'ZH_com_height': [], 'ZH_percent_above_45': [], 'ZH_percent_above_50': [], 'ZH_percent_above_55': [],\n",
    "        'KDP_com_height': [], 'KDP_percent_above_2': [], 'KDP_percent_above_1.5': [], 'KDP_percent_above_1': [],\n",
    "        'rad_shear_max': [], 'rad_shear_percent_above_2.5': [], 'rad_shear_percent_above_2': [], 'rad_shear_percent_above_1.5': [],\n",
    "        'area_p': []\n",
    "    }\n",
    "\n",
    "    for _, row in filtered_df.iterrows():\n",
    "        poly = row['geometry']\n",
    "        prep_poly = prep(poly)\n",
    "        minx, miny, maxx, maxy = poly.bounds\n",
    "        \n",
    "        # Bounding box optimization\n",
    "        bbox_mask = (clons_flat >= minx) & (clons_flat <= maxx) & \\\n",
    "                    (clats_flat >= miny) & (clats_flat <= maxy)\n",
    "        if not bbox_mask.any():\n",
    "            results = _append_defaults(results)\n",
    "            continue\n",
    "            \n",
    "        # Vectorized containment check\n",
    "        contained = np.array([prep_poly.contains(Point(p)) \n",
    "                               for p in zip(clons_flat[bbox_mask], clats_flat[bbox_mask])])\n",
    "        if not contained.any():\n",
    "            results = _append_defaults(results)\n",
    "            continue\n",
    "            \n",
    "        # Create 2D mask\n",
    "        mask_2d = np.zeros((ny, nx), bool)\n",
    "        yx_indices = np.unravel_index(np.where(bbox_mask)[0][contained], (ny, nx))\n",
    "        mask_2d[yx_indices] = True\n",
    "        \n",
    "        # Extend the mask to 3D by repeating along the vertical dimension\n",
    "        mask_3d = np.broadcast_to(mask_2d[..., None], (ny, nx, nz))\n",
    "        \n",
    "        # Extract values within the polygon for ZH, KDP, rad_shear\n",
    "        ZH_masked = np.where(mask_3d, ZH, np.nan)\n",
    "        KDP_masked = np.where(mask_3d, KDP, np.nan)\n",
    "        rad_shear_masked = np.where(mask_3d, rad_shear, np.nan)\n",
    "\n",
    "        # Replace NaN values with 0 for calculations\n",
    "        ZH_masked[np.isnan(ZH_masked)] = 0\n",
    "        KDP_masked[np.isnan(KDP_masked)] = 0\n",
    "        rad_shear_masked[np.isnan(rad_shear_masked)] = 0\n",
    "        \n",
    "        # Calculate metrics for ZH\n",
    "        if np.any(ZH_masked > 0):  # Check if there are valid values\n",
    "            com_ZH = center_of_mass(ZH_masked)  # Center of mass height\n",
    "            results['ZH_com_height'].append(com_ZH[2])  # Use the vertical dimension (z-axis)\n",
    "            results['ZH_percent_above_45'].append(np.sum(ZH_masked > 45) / np.size(ZH_masked) * 100)\n",
    "            results['ZH_percent_above_50'].append(np.sum(ZH_masked > 50) / np.size(ZH_masked) * 100)\n",
    "            results['ZH_percent_above_55'].append(np.sum(ZH_masked > 55) / np.size(ZH_masked) * 100)\n",
    "        else:\n",
    "            results['ZH_com_height'].append(np.nan)\n",
    "            results['ZH_percent_above_45'].append(0)\n",
    "            results['ZH_percent_above_50'].append(0)\n",
    "            results['ZH_percent_above_55'].append(0)\n",
    "        \n",
    "        # Calculate metrics for KDP\n",
    "        if np.any(KDP_masked > 0):  # Check if there are valid values\n",
    "            com_KDP = center_of_mass(KDP_masked)  # Center of mass height\n",
    "            results['KDP_com_height'].append(com_KDP[2])  # Use the vertical dimension (z-axis)\n",
    "            results['KDP_percent_above_2'].append(np.sum(KDP_masked > 2) / np.size(KDP_masked) * 100)\n",
    "            results['KDP_percent_above_1.5'].append(np.sum(KDP_masked > 1.5) / np.size(KDP_masked) * 100)\n",
    "            results['KDP_percent_above_1'].append(np.sum(KDP_masked > 1) / np.size(KDP_masked) * 100)\n",
    "        else:\n",
    "            results['KDP_com_height'].append(np.nan)\n",
    "            results['KDP_percent_above_2'].append(0)\n",
    "            results['KDP_percent_above_1.5'].append(0)\n",
    "            results['KDP_percent_above_1'].append(0)\n",
    "        \n",
    "        # Calculate metrics for rad_shear\n",
    "        if np.any(rad_shear_masked > 0):  # Check if there are valid values\n",
    "            results['rad_shear_max'].append(np.nanmax(rad_shear_masked))\n",
    "            results['rad_shear_percent_above_2.5'].append(np.sum(rad_shear_masked > 2.5) / np.size(rad_shear_masked) * 100)\n",
    "            results['rad_shear_percent_above_2'].append(np.sum(rad_shear_masked > 2) / np.size(rad_shear_masked) * 100)\n",
    "            results['rad_shear_percent_above_1.5'].append(np.sum(rad_shear_masked > 1.5) / np.size(rad_shear_masked) * 100)\n",
    "        else:\n",
    "            results['rad_shear_max'].append(np.nan)\n",
    "            results['rad_shear_percent_above_2.5'].append(0)\n",
    "            results['rad_shear_percent_above_2'].append(0)\n",
    "            results['rad_shear_percent_above_1.5'].append(0)\n",
    "\n",
    "    # Add results to DataFrame\n",
    "    for col in results:\n",
    "        filtered_df[col] = results[col]\n",
    "        \n",
    "    return filtered_df\n",
    "\n",
    "def _append_defaults(results):\n",
    "    \"\"\"Append default values to result lists when no valid data is found.\"\"\"\n",
    "    for k in results:\n",
    "        default = 0 if 'percent' in k or 'max' in k else np.nan\n",
    "        results[k].append(default)\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. Load Extraction Dates\n",
    "extraction_file = \"/scratch/mch/fackerma/orders/Reworked_gust_extraction_dates.txt\"\n",
    "\n",
    "# Read the file into a DataFrame\n",
    "extraction_dates = pd.read_csv(extraction_file)\n",
    "\n",
    "# Parse 'Valid_Time' column into datetime format\n",
    "extraction_dates['Valid_Time'] = pd.to_datetime(extraction_dates['Valid_Time'], format='%Y%m%d%H%M%S')\n",
    "\n",
    "# Extract valid times as Python datetime objects\n",
    "#valid_times = extraction_dates['Valid_Time'].dt.to_pydatetime()\n",
    "\n",
    "# Convert valid_times to timezone-aware datetime objects (UTC)\n",
    "valid_times = pd.to_datetime(extraction_dates['Valid_Time'], format='%Y%m%d%H%M%S').dt.tz_localize('UTC')\n",
    "\n",
    "\n",
    "\n",
    "# 2. Load Merged DataFrame\n",
    "base_dir = \"/scratch/mch/fackerma/orders/TRT_processing_3/\"\n",
    "yearly_files = [\n",
    "    \"TRT_2019_05-10.pkl\",\n",
    "    \"TRT_2020_05-10.pkl\",\n",
    "    \"TRT_2021_05-10.pkl\",\n",
    "    \"TRT_2022_05-10.pkl\",\n",
    "    \"TRT_2023_05-10.pkl\",\n",
    "]\n",
    "\n",
    "dfs = []\n",
    "for file_name in yearly_files:\n",
    "    file_path = os.path.join(base_dir, file_name)\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Loading {file_name}...\")\n",
    "        dfs.append(pd.read_pickle(file_path))\n",
    "\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# 3. Filter by Valid Times and Gust Flags\n",
    "# Convert merged_df timestamp to match extraction format\n",
    "merged_df['timestamp'] = pd.to_datetime(merged_df['yyyymmddHHMM'], utc=True)\n",
    "\n",
    "\n",
    "# Time filtering\n",
    "time_filter = merged_df['timestamp'].isin(valid_times)\n",
    "filtered_by_time = merged_df[time_filter].copy()\n",
    "print(f\"Number of rows after time filtering: {filtered_by_time.shape[0]}\")\n",
    "\n",
    "\n",
    "# Find traj_IDs with at least one Yes/No in Gust_Flag\n",
    "valid_traj_ids = merged_df[merged_df['Gust_Flag'].isin(['Yes', 'No'])]['traj_ID'].unique()\n",
    "traj_filter = filtered_by_time['traj_ID'].isin(valid_traj_ids)\n",
    "\n",
    "\n",
    "final_df = filtered_by_time[traj_filter].copy()\n",
    "print(f\"Number of rows after traj_ID filtering: {final_df.shape[0]}\")\n",
    "\n",
    "# 4. Process Data with calculate_metrics\n",
    "npz_base = '/scratch/mch/fackerma/orders/npz_GT2/full_composite_npz/'\n",
    "output_path = \"/scratch/mch/fackerma/orders/TRT_modelsetup_3/Model_Setup_3.pkl\"\n",
    "\n",
    "# Group by timestamp for NPZ loading\n",
    "grouped = final_df.groupby('timestamp')\n",
    "\n",
    "all_results = []\n",
    "for timestamp, group in grouped:\n",
    "    try:\n",
    "        # Load corresponding NPZ file\n",
    "        npz_time = timestamp.strftime('%Y%m%d%H%M00')\n",
    "        npz_path = f\"{npz_base}{npz_time}_conv_wind_composite_data_pl.npz\"\n",
    "        \n",
    "        if not os.path.exists(npz_path):\n",
    "            print(f\"⚠️ NPZ file not found: {npz_path}\")\n",
    "            continue\n",
    "            \n",
    "        with np.load(npz_path) as data:\n",
    "            ZH = data['ZH_max']\n",
    "            rad_shear = data['RAD_SHEAR_LLSD_max']\n",
    "            KDP = data['KDP_max']\n",
    "            \n",
    "        # Process the group\n",
    "        processed_group = calculate_metrics(\n",
    "            filtered_df=group,\n",
    "            clons=clons,\n",
    "            clats=clats,\n",
    "            ZH=ZH,\n",
    "            rad_shear=rad_shear,\n",
    "            KDP=KDP\n",
    "        )\n",
    "        \n",
    "        all_results.append(processed_group)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {timestamp}: {str(e)}\")\n",
    "\n",
    "# 5. Save Final Output\n",
    "if all_results:\n",
    "    final_output = pd.concat(all_results, ignore_index=True)\n",
    "    final_output.to_pickle(output_path)\n",
    "    print(f\"Saved final output to {output_path}\")\n",
    "else:\n",
    "    print(\"No data processed - output file not created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime64[ns, UTC]\n",
      "datetime64[ns, UTC]\n"
     ]
    }
   ],
   "source": [
    "print(merged_df['timestamp'].dtype)\n",
    "print(valid_times.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/mch/fackerma/orders/npz_GT2/full_composite_npz/20190511123500_conv_wind_composite_data_pl.npz'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npz_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-' 'Yes' 'No']\n"
     ]
    }
   ],
   "source": [
    "print(merged_df['Gust_Flag'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid traj_IDs: 3554\n",
      "['2019051112250040.0' '2019051112350035.0' '2019051112300050.0'\n",
      " '2019051113300051.0' '2019061509450011.0' '2019061510350021.0'\n",
      " '2019061515350046.0' '2019061518300167.0' '2019061519300003.0'\n",
      " '2019061518400144.0']\n"
     ]
    }
   ],
   "source": [
    "valid_traj_ids = merged_df[merged_df['Gust_Flag'].isin(['Yes', 'No'])]['traj_ID'].unique()\n",
    "print(f\"Number of valid traj_IDs: {len(valid_traj_ids)}\")\n",
    "print(valid_traj_ids[:10])  # Print first 10 valid traj_IDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after traj_ID filtering: 33066\n",
      "                 traj_ID Gust_Flag\n",
      "8125  2019051112250040.0         -\n",
      "8151  2019051112250040.0       Yes\n",
      "8153  2019051112300050.0         -\n",
      "8171  2019051112300050.0         -\n",
      "8172  2019051112350035.0         -\n"
     ]
    }
   ],
   "source": [
    "traj_filter = filtered_by_time['traj_ID'].isin(valid_traj_ids)\n",
    "filtered_by_traj = filtered_by_time[traj_filter].copy()\n",
    "print(f\"Number of rows after traj_ID filtering: {filtered_by_traj.shape[0]}\")\n",
    "print(filtered_by_traj[['traj_ID', 'Gust_Flag']].head())  # Inspect a few rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid timestamps: 14231\n",
      "0   2019-05-11 12:15:00+00:00\n",
      "1   2019-05-11 12:20:00+00:00\n",
      "2   2019-05-11 12:25:00+00:00\n",
      "3   2019-05-11 12:30:00+00:00\n",
      "4   2019-05-11 12:35:00+00:00\n",
      "Name: Valid_Time, dtype: datetime64[ns, UTC]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of valid timestamps: {len(valid_times)}\")\n",
    "print(valid_times[:5])  # Print the first 5 valid timestamps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of timestamps in merged_df: 150333\n",
      "0   2019-05-01 09:55:00+00:00\n",
      "1   2019-05-01 10:00:00+00:00\n",
      "2   2019-05-01 10:05:00+00:00\n",
      "3   2019-05-01 10:05:00+00:00\n",
      "4   2019-05-01 10:10:00+00:00\n",
      "Name: timestamp, dtype: datetime64[ns, UTC]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of timestamps in merged_df: {merged_df['timestamp'].nunique()}\")\n",
    "print(merged_df['timestamp'].head())  # Print the first 5 timestamps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully!\n",
      "timestamp                       0\n",
      "geometry                        0\n",
      "CS Marker                       0\n",
      "STA Marker                      0\n",
      "ESWD Marker                     0\n",
      "                               ..\n",
      "rad_shear_max                  37\n",
      "rad_shear_percent_above_2.5     0\n",
      "rad_shear_percent_above_2       0\n",
      "rad_shear_percent_above_1.5     0\n",
      "area_p                          0\n",
      "Length: 104, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = '/scratch/mch/fackerma/orders/TRT_modelsetup_2/Model_Setup_1.pkl'\n",
    "\n",
    "data = pd.read_pickle(file_path)\n",
    "print(\"File loaded successfully!\")\n",
    "print(data.isna().sum())  # Display NaN summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TRT_2019_05-10.pkl...\n",
      "Loading TRT_2020_05-10.pkl...\n",
      "Loading TRT_2021_05-10.pkl...\n",
      "Loading TRT_2022_05-10.pkl...\n",
      "Loading TRT_2023_05-10.pkl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mch/fackerma/miniforge3/envs/testenv/lib/python3.12/site-packages/geopandas/array.py:1638: UserWarning: CRS not set for some of the concatenation inputs. Setting output's CRS as WGS 84 (the single non-null crs provided).\n",
      "  return GeometryArray(data, crs=_get_common_crs(to_concat))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after time filtering: 297918\n",
      "Number of rows after traj_ID filtering: 18625\n",
      "Processed 2019-05-11 12:25:00+00:00\n",
      "Processed 2019-05-11 12:30:00+00:00\n",
      "Processed 2019-05-11 12:35:00+00:00\n",
      "Processed 2019-05-11 12:40:00+00:00\n",
      "Processed 2019-05-11 12:45:00+00:00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 220\u001b[0m\n\u001b[1;32m    217\u001b[0m     KDP \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKDP_max\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# Process the group\u001b[39;00m\n\u001b[0;32m--> 220\u001b[0m processed_group \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_metrics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclons\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclons\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mZH\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mZH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrad_shear\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrad_shear\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mKDP\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mKDP\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m all_results\u001b[38;5;241m.\u001b[39mappend(processed_group)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[16], line 74\u001b[0m, in \u001b[0;36mcalculate_metrics\u001b[0;34m(filtered_df, clons, clats, ZH, rad_shear, KDP)\u001b[0m\n\u001b[1;32m     71\u001b[0m mask_3d \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbroadcast_to(mask_2d[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m], (ny, nx, nz))\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Extract values within the polygon for ZH, KDP, rad_shear\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m ZH_masked \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask_3d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m KDP_masked \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(mask_3d, KDP, np\u001b[38;5;241m.\u001b[39mnan)\n\u001b[1;32m     76\u001b[0m rad_shear_masked \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(mask_3d, rad_shear, np\u001b[38;5;241m.\u001b[39mnan)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "from scipy.ndimage import center_of_mass\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyproj import Transformer\n",
    "from shapely.prepared import prep\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the Swiss grid (adjusted to match data dimensions)\n",
    "chx = np.arange(255000, 255000 + 710 * 1000, 1000)  # Easting values (710 points)\n",
    "chy = sorted(np.arange(-160000, -160000 + 640 * 1000, 1000), reverse=True)  # Northing values (640 points)\n",
    "X, Y = np.meshgrid(chx, chy)\n",
    "\n",
    "# Initialize transformer for Swiss grid to WGS84 (EPSG:21781 to EPSG:4326 PlateCarree)\n",
    "transformer = Transformer.from_crs(21781, 4326, always_xy=True)\n",
    "clons, clats = transformer.transform(X, Y)\n",
    "\n",
    "\n",
    "\n",
    "def calculate_metrics(filtered_df, clons, clats, ZH, rad_shear, KDP):\n",
    "    # Precompute grid properties\n",
    "    ny, nx = clons.shape\n",
    "    nz = ZH.shape[2]\n",
    "    clons_flat = clons.ravel()\n",
    "    clats_flat = clats.ravel()\n",
    "    \n",
    "    # Initialize results storage\n",
    "    results = {\n",
    "        'ZH_com_height': [], 'ZH_percent_above_45': [], 'ZH_percent_above_50': [], 'ZH_percent_above_55': [],\n",
    "        'KDP_com_height': [], 'KDP_percent_above_2': [], 'KDP_percent_above_1.5': [], 'KDP_percent_above_1': [],\n",
    "        'rad_shear_max': [], 'rad_shear_percent_above_2.5': [], 'rad_shear_percent_above_2': [], 'rad_shear_percent_above_1.5': [],\n",
    "        'area_p': []   # New column for polygon area\n",
    "    }\n",
    "\n",
    "    for _, row in filtered_df.iterrows():\n",
    "        poly = row['geometry']\n",
    "        \n",
    "        # Calculate polygon area in square meters\n",
    "        poly_area = poly.area  # Shapely's area property calculates area in native CRS units (meters for Swiss grid)\n",
    "        results['area_p'].append(poly_area)\n",
    "        \n",
    "        prep_poly = prep(poly)\n",
    "        minx, miny, maxx, maxy = poly.bounds\n",
    "        \n",
    "        # Bounding box optimization\n",
    "        bbox_mask = (clons_flat >= minx) & (clons_flat <= maxx) & \\\n",
    "                    (clats_flat >= miny) & (clats_flat <= maxy)\n",
    "        if not bbox_mask.any():\n",
    "            results = _append_defaults(results)\n",
    "            continue\n",
    "            \n",
    "        # Vectorized containment check\n",
    "        contained = np.array([prep_poly.contains(Point(p)) \n",
    "                               for p in zip(clons_flat[bbox_mask], clats_flat[bbox_mask])])\n",
    "        if not contained.any():\n",
    "            results = _append_defaults(results)\n",
    "            continue\n",
    "            \n",
    "        # Create 2D mask\n",
    "        mask_2d = np.zeros((ny, nx), bool)\n",
    "        yx_indices = np.unravel_index(np.where(bbox_mask)[0][contained], (ny, nx))\n",
    "        mask_2d[yx_indices] = True\n",
    "        \n",
    "        # Extend the mask to 3D by repeating along the vertical dimension\n",
    "        mask_3d = np.broadcast_to(mask_2d[..., None], (ny, nx, nz))\n",
    "        \n",
    "        # Extract values within the polygon for ZH, KDP, rad_shear\n",
    "        ZH_masked = np.where(mask_3d, ZH, np.nan)\n",
    "        KDP_masked = np.where(mask_3d, KDP, np.nan)\n",
    "        rad_shear_masked = np.where(mask_3d, rad_shear, np.nan)\n",
    "\n",
    "        # Replace NaN values with 0 for calculations\n",
    "        ZH_masked[np.isnan(ZH_masked)] = 0\n",
    "        KDP_masked[np.isnan(KDP_masked)] = 0\n",
    "        rad_shear_masked[np.isnan(rad_shear_masked)] = 0\n",
    "        \n",
    "        # Calculate metrics for ZH\n",
    "        if np.any(ZH_masked > 0):  # Check if there are valid values\n",
    "            com_ZH = center_of_mass(ZH_masked)  # Center of mass height\n",
    "            results['ZH_com_height'].append(com_ZH[2])  # Use the vertical dimension (z-axis)\n",
    "            results['ZH_percent_above_45'].append(np.sum(ZH_masked > 45) / np.size(ZH_masked) * 100)\n",
    "            results['ZH_percent_above_50'].append(np.sum(ZH_masked > 50) / np.size(ZH_masked) * 100)\n",
    "            results['ZH_percent_above_55'].append(np.sum(ZH_masked > 55) / np.size(ZH_masked) * 100)\n",
    "        else:\n",
    "            results['ZH_com_height'].append(np.nan)\n",
    "            results['ZH_percent_above_45'].append(0)\n",
    "            results['ZH_percent_above_50'].append(0)\n",
    "            results['ZH_percent_above_55'].append(0)\n",
    "        \n",
    "        # Calculate metrics for KDP\n",
    "        if np.any(KDP_masked > 0):  # Check if there are valid values\n",
    "            com_KDP = center_of_mass(KDP_masked)  # Center of mass height\n",
    "            results['KDP_com_height'].append(com_KDP[2])  # Use the vertical dimension (z-axis)\n",
    "            results['KDP_percent_above_2'].append(np.sum(KDP_masked > 2) / np.size(KDP_masked) * 100)\n",
    "            results['KDP_percent_above_1.5'].append(np.sum(KDP_masked > 1.5) / np.size(KDP_masked) * 100)\n",
    "            results['KDP_percent_above_1'].append(np.sum(KDP_masked > 1) / np.size(KDP_masked) * 100)\n",
    "        else:\n",
    "            results['KDP_com_height'].append(np.nan)\n",
    "            results['KDP_percent_above_2'].append(0)\n",
    "            results['KDP_percent_above_1.5'].append(0)\n",
    "            results['KDP_percent_above_1'].append(0)\n",
    "        \n",
    "        # Calculate metrics for rad_shear\n",
    "        if np.any(rad_shear_masked > 0):  # Check if there are valid values\n",
    "            results['rad_shear_max'].append(np.nanmax(rad_shear_masked))\n",
    "            results['rad_shear_percent_above_2.5'].append(np.sum(rad_shear_masked > 2.5) / np.size(rad_shear_masked) * 100)\n",
    "            results['rad_shear_percent_above_2'].append(np.sum(rad_shear_masked > 2) / np.size(rad_shear_masked) * 100)\n",
    "            results['rad_shear_percent_above_1.5'].append(np.sum(rad_shear_masked > 1.5) / np.size(rad_shear_masked) * 100)\n",
    "        else:\n",
    "            results['rad_shear_max'].append(np.nan)\n",
    "            results['rad_shear_percent_above_2.5'].append(0)\n",
    "            results['rad_shear_percent_above_2'].append(0)\n",
    "            results['rad_shear_percent_above_1.5'].append(0)\n",
    "\n",
    "    # Add results to DataFrame\n",
    "    for col in results:\n",
    "        filtered_df[col] = results[col]\n",
    "        \n",
    "    return filtered_df\n",
    "\n",
    "def _append_defaults(results):\n",
    "    \"\"\"Append default values to result lists when no valid data is found.\"\"\"\n",
    "    for k in results:\n",
    "        default = 0 if 'percent' in k or 'max' in k else np.nan\n",
    "        results[k].append(default)\n",
    "    return results \n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. Load Extraction Dates\n",
    "extraction_file = \"/scratch/mch/fackerma/orders/Extraction_dates_20250325130000.txt\"\n",
    "\n",
    "# Read the file into a DataFrame\n",
    "extraction_dates = pd.read_csv(extraction_file)\n",
    "\n",
    "# Parse 'Valid_Time' column into datetime format\n",
    "extraction_dates['Valid_Time'] = pd.to_datetime(extraction_dates['Valid_Time'], format='%Y%m%d%H%M%S')\n",
    "\n",
    "# Extract valid times as Python datetime objects\n",
    "#valid_times = extraction_dates['Valid_Time'].dt.to_pydatetime()\n",
    "\n",
    "# Convert valid_times to timezone-aware datetime objects (UTC)\n",
    "valid_times = pd.to_datetime(extraction_dates['Valid_Time'], format='%Y%m%d%H%M%S').dt.tz_localize('UTC')\n",
    "\n",
    "\n",
    "\n",
    "# 2. Load Merged DataFrame\n",
    "base_dir = \"/scratch/mch/fackerma/orders/TRT_processing_2/\"\n",
    "yearly_files = [\n",
    "    \"TRT_2019_05-10.pkl\",\n",
    "    \"TRT_2020_05-10.pkl\",\n",
    "    \"TRT_2021_05-10.pkl\",\n",
    "    \"TRT_2022_05-10.pkl\",\n",
    "    \"TRT_2023_05-10.pkl\",\n",
    "]\n",
    "\n",
    "dfs = []\n",
    "for file_name in yearly_files:\n",
    "    file_path = os.path.join(base_dir, file_name)\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Loading {file_name}...\")\n",
    "        dfs.append(pd.read_pickle(file_path))\n",
    "\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# 3. Filter by Valid Times and Gust Flags\n",
    "# Convert merged_df timestamp to match extraction format\n",
    "merged_df['timestamp'] = pd.to_datetime(merged_df['timestamp'], utc=True)\n",
    "\n",
    "\n",
    "# Time filtering\n",
    "time_filter = merged_df['timestamp'].isin(valid_times)\n",
    "filtered_by_time = merged_df[time_filter].copy()\n",
    "print(f\"Number of rows after time filtering: {filtered_by_time.shape[0]}\")\n",
    "\n",
    "\n",
    "# Find traj_IDs with at least one Yes/No in Gust_Flag\n",
    "valid_traj_ids = merged_df[merged_df['Gust_Flag'].isin(['Yes', 'No'])]['traj_ID'].unique()\n",
    "traj_filter = filtered_by_time['traj_ID'].isin(valid_traj_ids)\n",
    "\n",
    "\n",
    "final_df = filtered_by_time[traj_filter].copy()\n",
    "print(f\"Number of rows after traj_ID filtering: {final_df.shape[0]}\")\n",
    "\n",
    "# 4. Process Data with calculate_metrics\n",
    "npz_base = '/scratch/mch/maregger/hailclass/convective_wind/full_composite_npz/'\n",
    "output_path = \"/scratch/mch/fackerma/orders/TRT_modelsetup_2/Model_Setup_XX.pkl\"\n",
    "\n",
    "# Group by timestamp for NPZ loading\n",
    "grouped = final_df.groupby('timestamp')\n",
    "\n",
    "all_results = []\n",
    "for timestamp, group in grouped:\n",
    "    try:\n",
    "        # Load corresponding NPZ file\n",
    "        npz_time = timestamp.strftime('%Y%m%d%H%M00')\n",
    "        npz_path = f\"{npz_base}{npz_time}_conv_wind_composite_data.npz\"\n",
    "        \n",
    "        if not os.path.exists(npz_path):\n",
    "            print(f\"⚠️ NPZ file not found: {npz_path}\")\n",
    "            continue\n",
    "            \n",
    "        with np.load(npz_path) as data:\n",
    "            ZH = data['ZH_max']\n",
    "            rad_shear = data['RAD_SHEAR_LLSD_max']\n",
    "            KDP = data['KDP_max']\n",
    "            \n",
    "        # Process the group\n",
    "        processed_group = calculate_metrics(\n",
    "            filtered_df=group,\n",
    "            clons=clons,\n",
    "            clats=clats,\n",
    "            ZH=ZH,\n",
    "            rad_shear=rad_shear,\n",
    "            KDP=KDP\n",
    "        )\n",
    "        \n",
    "        all_results.append(processed_group)\n",
    "        print(f\"Processed {timestamp}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {timestamp}: {str(e)}\")\n",
    "\n",
    "# 5. Save Final Output\n",
    "if all_results:\n",
    "    final_output = pd.concat(all_results, ignore_index=True)\n",
    "    final_output.to_pickle(output_path)\n",
    "    print(f\"Saved final output to {output_path}\")\n",
    "else:\n",
    "    print(\"No data processed - output file not created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusted Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TRT_2019_05-10.pkl...\n",
      "Loading TRT_2020_05-10.pkl...\n",
      "Loading TRT_2021_05-10.pkl...\n",
      "Loading TRT_2022_05-10.pkl...\n",
      "Loading TRT_2023_05-10.pkl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mch/fackerma/miniforge3/envs/testenv/lib/python3.12/site-packages/geopandas/array.py:1638: UserWarning: CRS not set for some of the concatenation inputs. Setting output's CRS as WGS 84 (the single non-null crs provided).\n",
      "  return GeometryArray(data, crs=_get_common_crs(to_concat))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after time filtering: 416914\n",
      "Number of rows after traj_ID filtering: 33066\n",
      "Processed 2019-05-11 12:25:00+00:00\n",
      "Processed 2019-05-11 12:30:00+00:00\n",
      "Processed 2019-05-11 12:35:00+00:00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 280\u001b[0m\n\u001b[1;32m    277\u001b[0m     KDP \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKDP_max\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    279\u001b[0m \u001b[38;5;66;03m# Process the group\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m processed_group \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_metrics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclons\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclons\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mZH\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mZH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrad_shear\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrad_shear\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mKDP\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mKDP\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m all_results\u001b[38;5;241m.\u001b[39mappend(processed_group)\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 94\u001b[0m, in \u001b[0;36mcalculate_metrics\u001b[0;34m(filtered_df, clons, clats, ZH, rad_shear, KDP)\u001b[0m\n\u001b[1;32m     92\u001b[0m ZH_masked[np\u001b[38;5;241m.\u001b[39misnan(ZH_masked)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     93\u001b[0m KDP_masked[np\u001b[38;5;241m.\u001b[39misnan(KDP_masked)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 94\u001b[0m rad_shear_masked[\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misnan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrad_shear_masked\u001b[49m\u001b[43m)\u001b[49m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Calculate metrics for ZH\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(ZH_masked \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m):  \u001b[38;5;66;03m# Check if there are valid values\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "from scipy.ndimage import center_of_mass\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyproj import Transformer\n",
    "from shapely.prepared import prep\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the Swiss grid (adjusted to match data dimensions)\n",
    "chx = np.arange(255000, 255000 + 710 * 1000, 1000)  # Easting values (710 points)\n",
    "chy = sorted(np.arange(-160000, -160000 + 640 * 1000, 1000), reverse=True)  # Northing values (640 points)\n",
    "X, Y = np.meshgrid(chx, chy)\n",
    "\n",
    "# Initialize transformer for Swiss grid to WGS84 (EPSG:21781 to EPSG:4326 PlateCarree)\n",
    "transformer = Transformer.from_crs(21781, 4326, always_xy=True)\n",
    "clons, clats = transformer.transform(X, Y)\n",
    "\n",
    "\n",
    "\n",
    "def calculate_metrics(filtered_df, clons, clats, ZH, rad_shear, KDP):\n",
    "    # Precompute grid properties\n",
    "    ny, nx = clons.shape\n",
    "    nz = ZH.shape[2]\n",
    "    clons_flat = clons.ravel()\n",
    "    clats_flat = clats.ravel()\n",
    "    \n",
    "    # Initialize results storage\n",
    "    results = {\n",
    "        'ZH_com_height': [], 'ZH_percent_above_30': [], 'ZH_percent_above_35': [], 'ZH_percent_above_40': [], 'ZH_percent_above_45': [], 'ZH_percent_above_50': [], 'ZH_percent_above_55': [],\n",
    "        'KDP_com_height': [], 'KDP_percent_above_2': [], 'KDP_percent_above_1.5': [], 'KDP_percent_above_1': [], 'KDP_percent_above_0.5': [],\n",
    "        'rad_shear_max': [], 'rad_shear_percent_above_2.5': [], 'rad_shear_percent_above_2': [], 'rad_shear_percent_above_1.5': [], 'rad_shear_percent_above_1': [], 'rad_shear_percent_above_0.5': [],\n",
    "        'area_p': [],   \n",
    "        \n",
    "        # New ZH metrics\n",
    "        'ZH_45_height': [], 'ZH_20_height': [],\n",
    "        'ZH_95th_percentile': [], 'ZH_95th_percentile_height': [],\n",
    "        'ZH_max': [], 'ZH_max_height': [],\n",
    "    \n",
    "        # New KDP metrics\n",
    "        'KDP_95th_percentile': [], 'KDP_95th_percentile_height': [],\n",
    "        'KDP_max': [], 'KDP_max_height': [],\n",
    "    \n",
    "        # New rad_shear metrics\n",
    "        'rad_shear_95th_percentile': [], 'rad_shear_95th_percentile_height': [],\n",
    "        'rad_shear_max_height': []# New column for polygon area\n",
    "    }\n",
    "\n",
    "    for _, row in filtered_df.iterrows():\n",
    "        poly = row['geometry']\n",
    "        \n",
    "        # Calculate polygon area in square meters\n",
    "        poly_area = poly.area  # Shapely's area property calculates area in native CRS units (meters for Swiss grid)\n",
    "        results['area_p'].append(poly_area)\n",
    "        \n",
    "        prep_poly = prep(poly)\n",
    "        minx, miny, maxx, maxy = poly.bounds\n",
    "        \n",
    "        # Bounding box optimization\n",
    "        bbox_mask = (clons_flat >= minx) & (clons_flat <= maxx) & \\\n",
    "                    (clats_flat >= miny) & (clats_flat <= maxy)\n",
    "        if not bbox_mask.any():\n",
    "            results = _append_defaults(results)\n",
    "            continue\n",
    "            \n",
    "        # Vectorized containment check\n",
    "        contained = np.array([prep_poly.contains(Point(p)) \n",
    "                               for p in zip(clons_flat[bbox_mask], clats_flat[bbox_mask])])\n",
    "        if not contained.any():\n",
    "            results = _append_defaults(results)\n",
    "            continue\n",
    "            \n",
    "        # Create 2D mask\n",
    "        mask_2d = np.zeros((ny, nx), bool)\n",
    "        yx_indices = np.unravel_index(np.where(bbox_mask)[0][contained], (ny, nx))\n",
    "        mask_2d[yx_indices] = True\n",
    "        \n",
    "        # Extend the mask to 3D by repeating along the vertical dimension\n",
    "        mask_3d = np.broadcast_to(mask_2d[..., None], (ny, nx, nz))\n",
    "        \n",
    "        # Extract values within the polygon for ZH, KDP, rad_shear\n",
    "        ZH_masked = np.where(mask_3d, ZH, np.nan)\n",
    "        KDP_masked = np.where(mask_3d, KDP, np.nan)\n",
    "        rad_shear_masked = np.where(mask_3d, rad_shear, np.nan)\n",
    "\n",
    "        # Replace NaN values with 0 for calculations\n",
    "        ZH_masked[np.isnan(ZH_masked)] = 0\n",
    "        KDP_masked[np.isnan(KDP_masked)] = 0\n",
    "        rad_shear_masked[np.isnan(rad_shear_masked)] = 0\n",
    "        \n",
    "        # Calculate metrics for ZH\n",
    "        if np.any(ZH_masked > 0):  # Check if there are valid values\n",
    "            com_ZH = center_of_mass(ZH_masked)  # Center of mass height\n",
    "            results['ZH_com_height'].append(com_ZH[2])  # Use the vertical dimension (z-axis)\n",
    "            results['ZH_percent_above_30'].append(np.sum(ZH_masked > 30) / np.size(ZH_masked) * 100)\n",
    "            results['ZH_percent_above_35'].append(np.sum(ZH_masked > 35) / np.size(ZH_masked) * 100)\n",
    "            results['ZH_percent_above_40'].append(np.sum(ZH_masked > 40) / np.size(ZH_masked) * 100)\n",
    "            results['ZH_percent_above_45'].append(np.sum(ZH_masked > 45) / np.size(ZH_masked) * 100)\n",
    "            results['ZH_percent_above_50'].append(np.sum(ZH_masked > 50) / np.size(ZH_masked) * 100)\n",
    "            results['ZH_percent_above_55'].append(np.sum(ZH_masked > 55) / np.size(ZH_masked) * 100)\n",
    "            results['ZH_45_height'].append(np.max(np.where(ZH_masked >= 45)[2]) if np.any(ZH_masked >= 45) else np.nan)\n",
    "            results['ZH_20_height'].append(np.max(np.where(ZH_masked >= 20)[2]) if np.any(ZH_masked >= 20) else np.nan)\n",
    "            zh_95_val = np.percentile(ZH_masked[ZH_masked > 0], 95)\n",
    "            results['ZH_95th_percentile'].append(zh_95_val)\n",
    "            results['ZH_95th_percentile_height'].append(np.max(np.where(ZH_masked >= zh_95_val)[2]) if np.any(ZH_masked >= zh_95_val) else np.nan)\n",
    "            zh_max = np.nanmax(ZH_masked)\n",
    "            results['ZH_max'].append(zh_max)\n",
    "            results['ZH_max_height'].append(np.max(np.where(ZH_masked == zh_max)[2]) if np.any(ZH_masked == zh_max) else np.nan)\n",
    "\n",
    "        else:\n",
    "            results['ZH_com_height'].append(np.nan)\n",
    "            results['ZH_percent_above_30'].append(0)\n",
    "            results['ZH_percent_above_35'].append(0)\n",
    "            results['ZH_percent_above_40'].append(0)\n",
    "            results['ZH_percent_above_45'].append(0)\n",
    "            results['ZH_percent_above_50'].append(0)\n",
    "            results['ZH_percent_above_55'].append(0)\n",
    "            results['ZH_45_height'].append(np.nan)\n",
    "            results['ZH_20_height'].append(np.nan)\n",
    "            results['ZH_95th_percentile'].append(0)\n",
    "            results['ZH_95th_percentile_height'].append(np.nan)\n",
    "            results['ZH_max'].append(0)\n",
    "            results['ZH_max_height'].append(np.nan)    \n",
    "        \n",
    "        # Calculate metrics for KDP\n",
    "        if np.any(KDP_masked > 0):  # Check if there are valid values\n",
    "            com_KDP = center_of_mass(KDP_masked)  # Center of mass height\n",
    "            results['KDP_com_height'].append(com_KDP[2])  # Use the vertical dimension (z-axis)\n",
    "            results['KDP_percent_above_2'].append(np.sum(KDP_masked > 2) / np.size(KDP_masked) * 100)\n",
    "            results['KDP_percent_above_1.5'].append(np.sum(KDP_masked > 1.5) / np.size(KDP_masked) * 100)\n",
    "            results['KDP_percent_above_1'].append(np.sum(KDP_masked > 1) / np.size(KDP_masked) * 100)\n",
    "            results['KDP_percent_above_0.5'].append(np.sum(KDP_masked > 0.5) / np.size(KDP_masked) * 100)\n",
    "            kdp_95_val = np.percentile(KDP_masked[KDP_masked > 0], 95)\n",
    "            results['KDP_95th_percentile'].append(kdp_95_val)\n",
    "            results['KDP_95th_percentile_height'].append(np.max(np.where(KDP_masked >= kdp_95_val)[2]) if np.any(KDP_masked >= kdp_95_val) else np.nan)\n",
    "            kdp_max = np.nanmax(KDP_masked)\n",
    "            results['KDP_max'].append(kdp_max)\n",
    "            results['KDP_max_height'].append(np.max(np.where(KDP_masked == kdp_max)[2]) if np.any(KDP_masked == kdp_max) else np.nan)\n",
    "\n",
    "        else:\n",
    "            results['KDP_com_height'].append(np.nan)\n",
    "            results['KDP_percent_above_2'].append(0)\n",
    "            results['KDP_percent_above_1.5'].append(0)\n",
    "            results['KDP_percent_above_1'].append(0)\n",
    "            results['KDP_percent_above_0.5'].append(0)\n",
    "            results['KDP_95th_percentile'].append(0)\n",
    "            results['KDP_95th_percentile_height'].append(np.nan)\n",
    "            results['KDP_max'].append(0)\n",
    "            results['KDP_max_height'].append(np.nan)\n",
    "        \n",
    "        # Calculate metrics for rad_shear\n",
    "        if np.any(rad_shear_masked > 0):  # Check if there are valid values\n",
    "            results['rad_shear_max'].append(np.nanmax(rad_shear_masked))\n",
    "            results['rad_shear_percent_above_2.5'].append(np.sum(rad_shear_masked > 2.5) / np.size(rad_shear_masked) * 100)\n",
    "            results['rad_shear_percent_above_2'].append(np.sum(rad_shear_masked > 2) / np.size(rad_shear_masked) * 100)\n",
    "            results['rad_shear_percent_above_1.5'].append(np.sum(rad_shear_masked > 1.5) / np.size(rad_shear_masked) * 100)\n",
    "            results['rad_shear_percent_above_1'].append(np.sum(rad_shear_masked > 1) / np.size(rad_shear_masked) * 100)\n",
    "            results['rad_shear_percent_above_0.5'].append(np.sum(rad_shear_masked > 0.5) / np.size(rad_shear_masked) * 100)\n",
    "            rs_95_val = np.percentile(rad_shear_masked[rad_shear_masked > 0], 95)\n",
    "            results['rad_shear_95th_percentile'].append(rs_95_val)\n",
    "            results['rad_shear_95th_percentile_height'].append(np.max(np.where(rad_shear_masked >= rs_95_val)[2]) if np.any(rad_shear_masked >= rs_95_val) else np.nan)\n",
    "            rs_max = results['rad_shear_max'][-1]  # From existing max calculation\n",
    "            results['rad_shear_max_height'].append(np.max(np.where(rad_shear_masked == rs_max)[2]) if not np.isnan(rs_max) else np.nan)\n",
    "\n",
    "        else:\n",
    "            results['rad_shear_max'].append(np.nan)\n",
    "            results['rad_shear_percent_above_2.5'].append(0)\n",
    "            results['rad_shear_percent_above_2'].append(0)\n",
    "            results['rad_shear_percent_above_1.5'].append(0)\n",
    "            results['rad_shear_percent_above_1'].append(0)\n",
    "            results['rad_shear_percent_above_0.5'].append(0)\n",
    "            results['rad_shear_95th_percentile'].append(0)\n",
    "            results['rad_shear_95th_percentile_height'].append(np.nan)\n",
    "            results['rad_shear_max_height'].append(np.nan)\n",
    "\n",
    "    # Add results to DataFrame\n",
    "    for col in results:\n",
    "        filtered_df[col] = results[col]\n",
    "        \n",
    "    return filtered_df\n",
    "\n",
    "def _append_defaults(results):\n",
    "    \"\"\"Append default values to result lists when no valid data is found.\"\"\"\n",
    "    for k in results:\n",
    "        default = 0 if 'percent' in k or 'max' in k else np.nan\n",
    "        results[k].append(default)\n",
    "    return results \n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. Load Extraction Dates\n",
    "extraction_file = \"/scratch/mch/fackerma/orders/Reworked_gust_extraction_dates.txt\"\n",
    "\n",
    "# Read the file into a DataFrame\n",
    "extraction_dates = pd.read_csv(extraction_file)\n",
    "\n",
    "# Parse 'Valid_Time' column into datetime format\n",
    "extraction_dates['Valid_Time'] = pd.to_datetime(extraction_dates['Valid_Time'], format='%Y%m%d%H%M%S')\n",
    "\n",
    "# Extract valid times as Python datetime objects\n",
    "#valid_times = extraction_dates['Valid_Time'].dt.to_pydatetime()\n",
    "\n",
    "# Convert valid_times to timezone-aware datetime objects (UTC)\n",
    "valid_times = pd.to_datetime(extraction_dates['Valid_Time'], format='%Y%m%d%H%M%S').dt.tz_localize('UTC')\n",
    "\n",
    "\n",
    "\n",
    "# 2. Load Merged DataFrame\n",
    "base_dir = \"/scratch/mch/fackerma/orders/TRT_processing_3/\"\n",
    "yearly_files = [\n",
    "    \"TRT_2019_05-10.pkl\",\n",
    "    \"TRT_2020_05-10.pkl\",\n",
    "    \"TRT_2021_05-10.pkl\",\n",
    "    \"TRT_2022_05-10.pkl\",\n",
    "    \"TRT_2023_05-10.pkl\",\n",
    "]\n",
    "\n",
    "dfs = []\n",
    "for file_name in yearly_files:\n",
    "    file_path = os.path.join(base_dir, file_name)\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Loading {file_name}...\")\n",
    "        dfs.append(pd.read_pickle(file_path))\n",
    "\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# 3. Filter by Valid Times and Gust Flags\n",
    "# Convert merged_df timestamp to match extraction format\n",
    "merged_df['timestamp'] = pd.to_datetime(merged_df['yyyymmddHHMM'], utc=True)\n",
    "\n",
    "\n",
    "# Time filtering\n",
    "time_filter = merged_df['timestamp'].isin(valid_times)\n",
    "filtered_by_time = merged_df[time_filter].copy()\n",
    "print(f\"Number of rows after time filtering: {filtered_by_time.shape[0]}\")\n",
    "\n",
    "\n",
    "# Find traj_IDs with at least one Yes/No in Gust_Flag\n",
    "valid_traj_ids = merged_df[merged_df['Gust_Flag'].isin(['Yes', 'No'])]['traj_ID'].unique()\n",
    "traj_filter = filtered_by_time['traj_ID'].isin(valid_traj_ids)\n",
    "\n",
    "\n",
    "final_df = filtered_by_time[traj_filter].copy()\n",
    "print(f\"Number of rows after traj_ID filtering: {final_df.shape[0]}\")\n",
    "\n",
    "# 4. Process Data with calculate_metrics\n",
    "npz_base = '/scratch/mch/maregger/hailclass/convective_wind/full_composite_npz/'\n",
    "output_path = \"/scratch/mch/fackerma/orders/TRT_modelsetup_3/Model_Setup_XX.pkl\"\n",
    "\n",
    "# Group by timestamp for NPZ loading\n",
    "grouped = final_df.groupby('timestamp')\n",
    "\n",
    "all_results = []\n",
    "for timestamp, group in grouped:\n",
    "    try:\n",
    "        # Load corresponding NPZ file\n",
    "        npz_time = timestamp.strftime('%Y%m%d%H%M00')\n",
    "        npz_path = f\"{npz_base}{npz_time}_conv_wind_composite_data_pl.npz\"\n",
    "        \n",
    "        if not os.path.exists(npz_path):\n",
    "            print(f\"⚠️ NPZ file not found: {npz_path}\")\n",
    "            continue\n",
    "            \n",
    "        with np.load(npz_path) as data:\n",
    "            ZH = data['ZH_max']\n",
    "            rad_shear = data['RAD_SHEAR_LLSD_max']\n",
    "            KDP = data['KDP_max']\n",
    "            \n",
    "        # Process the group\n",
    "        processed_group = calculate_metrics(\n",
    "            filtered_df=group,\n",
    "            clons=clons,\n",
    "            clats=clats,\n",
    "            ZH=ZH,\n",
    "            rad_shear=rad_shear,\n",
    "            KDP=KDP\n",
    "        )\n",
    "        \n",
    "        all_results.append(processed_group)\n",
    "        print(f\"Processed {timestamp}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {timestamp}: {str(e)}\")\n",
    "\n",
    "# 5. Save Final Output\n",
    "if all_results:\n",
    "    final_output = pd.concat(all_results, ignore_index=True)\n",
    "    final_output.to_pickle(output_path)\n",
    "    print(f\"Saved final output to {output_path}\")\n",
    "else:\n",
    "    print(\"No data processed - output file not created\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
